{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Transformer Attention for Abstracts.ipynb",
      "provenance": [],
      "mount_file_id": "1wrNrrh681EXHNIKUYwioNtFjK7g9ML4B",
      "authorship_tag": "ABX9TyPhXYS+8fqqTXsIAwZpmuIY"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "jCumT6vn4rTy"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import time\n",
        "import re\n",
        "import pickle"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 564
        },
        "id": "wdQgbAtM4wLw",
        "outputId": "264fa402-c51e-4b94-9dca-8e56b8f3cce0"
      },
      "source": [
        "import pandas as pd\n",
        "df=pd.read_csv(\"/content/drive/MyDrive/metadata.csv\",low_memory=False)\n",
        "df.head()"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>cord_uid</th>\n",
              "      <th>sha</th>\n",
              "      <th>source_x</th>\n",
              "      <th>title</th>\n",
              "      <th>doi</th>\n",
              "      <th>pmcid</th>\n",
              "      <th>pubmed_id</th>\n",
              "      <th>license</th>\n",
              "      <th>abstract</th>\n",
              "      <th>publish_time</th>\n",
              "      <th>authors</th>\n",
              "      <th>journal</th>\n",
              "      <th>mag_id</th>\n",
              "      <th>who_covidence_id</th>\n",
              "      <th>arxiv_id</th>\n",
              "      <th>pdf_json_files</th>\n",
              "      <th>pmc_json_files</th>\n",
              "      <th>url</th>\n",
              "      <th>s2_id</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>ug7v899j</td>\n",
              "      <td>d1aafb70c066a2068b02786f8929fd9c900897fb</td>\n",
              "      <td>PMC</td>\n",
              "      <td>Clinical features of culture-proven Mycoplasma...</td>\n",
              "      <td>10.1186/1471-2334-1-6</td>\n",
              "      <td>PMC35282</td>\n",
              "      <td>11472636</td>\n",
              "      <td>no-cc</td>\n",
              "      <td>OBJECTIVE: This retrospective chart review des...</td>\n",
              "      <td>2001-07-04</td>\n",
              "      <td>Madani, Tariq A; Al-Ghamdi, Aisha A</td>\n",
              "      <td>BMC Infect Dis</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>document_parses/pdf_json/d1aafb70c066a2068b027...</td>\n",
              "      <td>document_parses/pmc_json/PMC35282.xml.json</td>\n",
              "      <td>https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3...</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>02tnwd4m</td>\n",
              "      <td>6b0567729c2143a66d737eb0a2f63f2dce2e5a7d</td>\n",
              "      <td>PMC</td>\n",
              "      <td>Nitric oxide: a pro-inflammatory mediator in l...</td>\n",
              "      <td>10.1186/rr14</td>\n",
              "      <td>PMC59543</td>\n",
              "      <td>11667967</td>\n",
              "      <td>no-cc</td>\n",
              "      <td>Inflammatory diseases of the respiratory tract...</td>\n",
              "      <td>2000-08-15</td>\n",
              "      <td>Vliet, Albert van der; Eiserich, Jason P; Cros...</td>\n",
              "      <td>Respir Res</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>document_parses/pdf_json/6b0567729c2143a66d737...</td>\n",
              "      <td>document_parses/pmc_json/PMC59543.xml.json</td>\n",
              "      <td>https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5...</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>ejv2xln0</td>\n",
              "      <td>06ced00a5fc04215949aa72528f2eeaae1d58927</td>\n",
              "      <td>PMC</td>\n",
              "      <td>Surfactant protein-D and pulmonary host defense</td>\n",
              "      <td>10.1186/rr19</td>\n",
              "      <td>PMC59549</td>\n",
              "      <td>11667972</td>\n",
              "      <td>no-cc</td>\n",
              "      <td>Surfactant protein-D (SP-D) participates in th...</td>\n",
              "      <td>2000-08-25</td>\n",
              "      <td>Crouch, Erika C</td>\n",
              "      <td>Respir Res</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>document_parses/pdf_json/06ced00a5fc04215949aa...</td>\n",
              "      <td>document_parses/pmc_json/PMC59549.xml.json</td>\n",
              "      <td>https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5...</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>2b73a28n</td>\n",
              "      <td>348055649b6b8cf2b9a376498df9bf41f7123605</td>\n",
              "      <td>PMC</td>\n",
              "      <td>Role of endothelin-1 in lung disease</td>\n",
              "      <td>10.1186/rr44</td>\n",
              "      <td>PMC59574</td>\n",
              "      <td>11686871</td>\n",
              "      <td>no-cc</td>\n",
              "      <td>Endothelin-1 (ET-1) is a 21 amino acid peptide...</td>\n",
              "      <td>2001-02-22</td>\n",
              "      <td>Fagan, Karen A; McMurtry, Ivan F; Rodman, David M</td>\n",
              "      <td>Respir Res</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>document_parses/pdf_json/348055649b6b8cf2b9a37...</td>\n",
              "      <td>document_parses/pmc_json/PMC59574.xml.json</td>\n",
              "      <td>https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5...</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>9785vg6d</td>\n",
              "      <td>5f48792a5fa08bed9f56016f4981ae2ca6031b32</td>\n",
              "      <td>PMC</td>\n",
              "      <td>Gene expression in epithelial cells in respons...</td>\n",
              "      <td>10.1186/rr61</td>\n",
              "      <td>PMC59580</td>\n",
              "      <td>11686888</td>\n",
              "      <td>no-cc</td>\n",
              "      <td>Respiratory syncytial virus (RSV) and pneumoni...</td>\n",
              "      <td>2001-05-11</td>\n",
              "      <td>Domachowske, Joseph B; Bonville, Cynthia A; Ro...</td>\n",
              "      <td>Respir Res</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>document_parses/pdf_json/5f48792a5fa08bed9f560...</td>\n",
              "      <td>document_parses/pmc_json/PMC59580.xml.json</td>\n",
              "      <td>https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5...</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   cord_uid  ... s2_id\n",
              "0  ug7v899j  ...   NaN\n",
              "1  02tnwd4m  ...   NaN\n",
              "2  ejv2xln0  ...   NaN\n",
              "3  2b73a28n  ...   NaN\n",
              "4  9785vg6d  ...   NaN\n",
              "\n",
              "[5 rows x 19 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z7HQRuHPSLuG",
        "outputId": "788f7bc1-9d53-4433-a05c-ca9c48bfcc89"
      },
      "source": [
        "df.info()"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 476776 entries, 0 to 476775\n",
            "Data columns (total 19 columns):\n",
            " #   Column            Non-Null Count   Dtype  \n",
            "---  ------            --------------   -----  \n",
            " 0   cord_uid          476776 non-null  object \n",
            " 1   sha               157554 non-null  object \n",
            " 2   source_x          476776 non-null  object \n",
            " 3   title             476544 non-null  object \n",
            " 4   doi               262674 non-null  object \n",
            " 5   pmcid             166958 non-null  object \n",
            " 6   pubmed_id         229761 non-null  object \n",
            " 7   license           476776 non-null  object \n",
            " 8   abstract          344732 non-null  object \n",
            " 9   publish_time      476557 non-null  object \n",
            " 10  authors           463300 non-null  object \n",
            " 11  journal           445028 non-null  object \n",
            " 12  mag_id            0 non-null       float64\n",
            " 13  who_covidence_id  196150 non-null  object \n",
            " 14  arxiv_id          6269 non-null    object \n",
            " 15  pdf_json_files    157554 non-null  object \n",
            " 16  pmc_json_files    126480 non-null  object \n",
            " 17  url               282240 non-null  object \n",
            " 18  s2_id             434622 non-null  float64\n",
            "dtypes: float64(2), object(17)\n",
            "memory usage: 69.1+ MB\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-fDVBEx45YY5"
      },
      "source": [
        "df=df[['title','abstract']]"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OmUSsDniSsOP"
      },
      "source": [
        "df=df[:40000]"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BANh-fna5bCl"
      },
      "source": [
        "df.dropna(inplace=True)"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qip4jNDT41e3"
      },
      "source": [
        "document = df['abstract']\n",
        "summary = df['title']"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YYMCbOGR5CEC",
        "outputId": "247ce4df-5143-4427-ba1a-103dc8cf27a8"
      },
      "source": [
        "document[3], summary[3]"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('Endothelin-1 (ET-1) is a 21 amino acid peptide with diverse biological activity that has been implicated in numerous diseases. ET-1 is a potent mitogen regulator of smooth muscle tone, and inflammatory mediator that may play a key role in diseases of the airways, pulmonary circulation, and inflammatory lung diseases, both acute and chronic. This review will focus on the biology of ET-1 and its role in lung disease.',\n",
              " 'Role of endothelin-1 in lung disease')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h-qHXIcy5GvW",
        "outputId": "4b24de07-94cc-419b-d4ad-3803200014b4"
      },
      "source": [
        "summary = summary.apply(lambda x: '<go> ' + x + ' <stop>')\n",
        "summary.head()"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0    <go> Clinical features of culture-proven Mycop...\n",
              "1    <go> Nitric oxide: a pro-inflammatory mediator...\n",
              "2    <go> Surfactant protein-D and pulmonary host d...\n",
              "3     <go> Role of endothelin-1 in lung disease <stop>\n",
              "4    <go> Gene expression in epithelial cells in re...\n",
              "Name: title, dtype: object"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fm8mqzcI5Tdq"
      },
      "source": [
        "# since < and > from default tokens cannot be removed\n",
        "filters = '!\"#$%&()*+,-./:;=?@[\\\\]^_`{|}~\\t\\n'\n",
        "oov_token = '<unk>'"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PGjwQ9yn5XYj"
      },
      "source": [
        "document_tokenizer = tf.keras.preprocessing.text.Tokenizer(oov_token=oov_token)\n",
        "summary_tokenizer = tf.keras.preprocessing.text.Tokenizer(filters=filters, oov_token=oov_token)"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A-a8cGIM5aN_"
      },
      "source": [
        "document_tokenizer.fit_on_texts(document)\n",
        "summary_tokenizer.fit_on_texts(summary)"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HHoFzlKF5fFK"
      },
      "source": [
        "\n",
        "inputs = document_tokenizer.texts_to_sequences(document)\n",
        "targets = summary_tokenizer.texts_to_sequences(summary)\n"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-dap8PjG5iB4",
        "outputId": "84703913-22ed-4fd0-af29-a5b07d97e7cf"
      },
      "source": [
        "summary_tokenizer.texts_to_sequences([\"This is a test\"])"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[2961, 37, 8, 688]]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o3cbowDR5kdz",
        "outputId": "b764079e-9c14-47d1-c4e5-85d358b5c288"
      },
      "source": [
        "summary_tokenizer.sequences_to_texts([[20, 13, 7, 158]])"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['an influenza the vaccines']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iLmMzTSe5nXF",
        "outputId": "adf95d4e-2789-47e9-cdfe-252518f92695"
      },
      "source": [
        "encoder_vocab_size = len(document_tokenizer.word_index) + 1\n",
        "decoder_vocab_size = len(summary_tokenizer.word_index) + 1\n",
        "\n",
        "# vocab_size\n",
        "encoder_vocab_size, decoder_vocab_size"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(50111, 13030)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z2aWM3Uy5p_E"
      },
      "source": [
        "document_lengths = pd.Series([len(x) for x in document])\n",
        "summary_lengths = pd.Series([len(x) for x in summary])"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SWqa5Ot05t6m",
        "outputId": "f789ae65-8e0f-4493-c828-ef749bb78429"
      },
      "source": [
        "document_lengths.describe()"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "count     7006.000000\n",
              "mean      1426.205253\n",
              "std       1155.378613\n",
              "min          7.000000\n",
              "25%       1070.000000\n",
              "50%       1398.000000\n",
              "75%       1739.000000\n",
              "max      81020.000000\n",
              "dtype: float64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zg2g43RN5vWB",
        "outputId": "2561d99c-5ee7-48f5-b679-48271454b318"
      },
      "source": [
        "summary_lengths.describe()"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "count    7006.000000\n",
              "mean      112.837853\n",
              "std        37.858148\n",
              "min        21.000000\n",
              "25%        86.000000\n",
              "50%       111.000000\n",
              "75%       137.000000\n",
              "max       349.000000\n",
              "dtype: float64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lo5yUqQg5w78"
      },
      "source": [
        "encoder_maxlen = 1000\n",
        "decoder_maxlen = 110"
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fzfazI78564-"
      },
      "source": [
        "\n",
        "inputs = tf.keras.preprocessing.sequence.pad_sequences(inputs, maxlen=encoder_maxlen, padding='post', truncating='post')\n",
        "targets = tf.keras.preprocessing.sequence.pad_sequences(targets, maxlen=decoder_maxlen, padding='post', truncating='post')"
      ],
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XLBfgUxX6s-j"
      },
      "source": [
        "\n",
        "inputs = tf.cast(inputs, dtype=tf.int32)\n",
        "targets = tf.cast(targets, dtype=tf.int32)"
      ],
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XzarlMLU6899"
      },
      "source": [
        "BUFFER_SIZE = 20000\n",
        "BATCH_SIZE = 16"
      ],
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OCPZFusD6_zG"
      },
      "source": [
        "dataset = tf.data.Dataset.from_tensor_slices((inputs, targets)).shuffle(BUFFER_SIZE).batch(BATCH_SIZE)"
      ],
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zoij2pKR7Hiu"
      },
      "source": [
        "def get_angles(position, i, d_model):\n",
        "    angle_rates = 1 / np.power(10000, (2 * (i // 2)) / np.float32(d_model))\n",
        "    return position * angle_rates"
      ],
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "32ogs4Hc7KDO"
      },
      "source": [
        "def positional_encoding(position, d_model):\n",
        "    angle_rads = get_angles(\n",
        "        np.arange(position)[:, np.newaxis],\n",
        "        np.arange(d_model)[np.newaxis, :],\n",
        "        d_model\n",
        "    )\n",
        "\n",
        "    # apply sin to even indices in the array; 2i\n",
        "    angle_rads[:, 0::2] = np.sin(angle_rads[:, 0::2])\n",
        "\n",
        "    # apply cos to odd indices in the array; 2i+1\n",
        "    angle_rads[:, 1::2] = np.cos(angle_rads[:, 1::2])\n",
        "\n",
        "    pos_encoding = angle_rads[np.newaxis, ...]\n",
        "\n",
        "    return tf.cast(pos_encoding, dtype=tf.float32)"
      ],
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oDIjp5QO7N9_"
      },
      "source": [
        "def create_padding_mask(seq):\n",
        "    seq = tf.cast(tf.math.equal(seq, 0), tf.float32)\n",
        "    return seq[:, tf.newaxis, tf.newaxis, :]"
      ],
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ukKhmxF17QdP"
      },
      "source": [
        "def create_look_ahead_mask(size):\n",
        "    mask = 1 - tf.linalg.band_part(tf.ones((size, size)), -1, 0)\n",
        "    return mask"
      ],
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DbiZN34f7U9M"
      },
      "source": [
        "def scaled_dot_product_attention(q, k, v, mask):\n",
        "    matmul_qk = tf.matmul(q, k, transpose_b=True)\n",
        "\n",
        "    dk = tf.cast(tf.shape(k)[-1], tf.float32)\n",
        "    scaled_attention_logits = matmul_qk / tf.math.sqrt(dk)\n",
        "\n",
        "    if mask is not None:\n",
        "        scaled_attention_logits += (mask * -1e9)  \n",
        "\n",
        "    attention_weights = tf.nn.softmax(scaled_attention_logits, axis=-1)\n",
        "\n",
        "    output = tf.matmul(attention_weights, v)\n",
        "    return output, attention_weights"
      ],
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yVte0TdD7Y_p"
      },
      "source": [
        "class MultiHeadAttention(tf.keras.layers.Layer):\n",
        "    def __init__(self, d_model, num_heads):\n",
        "        super(MultiHeadAttention, self).__init__()\n",
        "        self.num_heads = num_heads\n",
        "        self.d_model = d_model\n",
        "\n",
        "        assert d_model % self.num_heads == 0\n",
        "\n",
        "        self.depth = d_model // self.num_heads\n",
        "\n",
        "        self.wq = tf.keras.layers.Dense(d_model)\n",
        "        self.wk = tf.keras.layers.Dense(d_model)\n",
        "        self.wv = tf.keras.layers.Dense(d_model)\n",
        "\n",
        "        self.dense = tf.keras.layers.Dense(d_model)\n",
        "        \n",
        "    def split_heads(self, x, batch_size):\n",
        "        x = tf.reshape(x, (batch_size, -1, self.num_heads, self.depth))\n",
        "        return tf.transpose(x, perm=[0, 2, 1, 3])\n",
        "    \n",
        "    def call(self, v, k, q, mask):\n",
        "        batch_size = tf.shape(q)[0]\n",
        "\n",
        "        q = self.wq(q)\n",
        "        k = self.wk(k)\n",
        "        v = self.wv(v)\n",
        "\n",
        "        q = self.split_heads(q, batch_size)\n",
        "        k = self.split_heads(k, batch_size)\n",
        "        v = self.split_heads(v, batch_size)\n",
        "\n",
        "        scaled_attention, attention_weights = scaled_dot_product_attention(\n",
        "            q, k, v, mask)\n",
        "\n",
        "        scaled_attention = tf.transpose(scaled_attention, perm=[0, 2, 1, 3])\n",
        "\n",
        "        concat_attention = tf.reshape(scaled_attention, (batch_size, -1, self.d_model))\n",
        "        output = self.dense(concat_attention)\n",
        "            \n",
        "        return output, attention_weights\n"
      ],
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "39FX7E4R7cHm"
      },
      "source": [
        "def point_wise_feed_forward_network(d_model, dff):\n",
        "    return tf.keras.Sequential([\n",
        "        tf.keras.layers.Dense(dff, activation='relu'),\n",
        "        tf.keras.layers.Dense(d_model)\n",
        "    ])"
      ],
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-nNJRVM07ebd"
      },
      "source": [
        "class EncoderLayer(tf.keras.layers.Layer):\n",
        "    def __init__(self, d_model, num_heads, dff, rate=0.1):\n",
        "        super(EncoderLayer, self).__init__()\n",
        "\n",
        "        self.mha = MultiHeadAttention(d_model, num_heads)\n",
        "        self.ffn = point_wise_feed_forward_network(d_model, dff)\n",
        "\n",
        "        self.layernorm1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
        "        self.layernorm2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
        "\n",
        "        self.dropout1 = tf.keras.layers.Dropout(rate)\n",
        "        self.dropout2 = tf.keras.layers.Dropout(rate)\n",
        "    \n",
        "    def call(self, x, training, mask):\n",
        "        attn_output, _ = self.mha(x, x, x, mask)\n",
        "        attn_output = self.dropout1(attn_output, training=training)\n",
        "        out1 = self.layernorm1(x + attn_output)\n",
        "\n",
        "        ffn_output = self.ffn(out1)\n",
        "        ffn_output = self.dropout2(ffn_output, training=training)\n",
        "        out2 = self.layernorm2(out1 + ffn_output)\n",
        "\n",
        "        return out2"
      ],
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZEJg8pJA7guR"
      },
      "source": [
        "class DecoderLayer(tf.keras.layers.Layer):\n",
        "    def __init__(self, d_model, num_heads, dff, rate=0.1):\n",
        "        super(DecoderLayer, self).__init__()\n",
        "\n",
        "        self.mha1 = MultiHeadAttention(d_model, num_heads)\n",
        "        self.mha2 = MultiHeadAttention(d_model, num_heads)\n",
        "\n",
        "        self.ffn = point_wise_feed_forward_network(d_model, dff)\n",
        "\n",
        "        self.layernorm1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
        "        self.layernorm2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
        "        self.layernorm3 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
        "\n",
        "        self.dropout1 = tf.keras.layers.Dropout(rate)\n",
        "        self.dropout2 = tf.keras.layers.Dropout(rate)\n",
        "        self.dropout3 = tf.keras.layers.Dropout(rate)\n",
        "    \n",
        "    \n",
        "    def call(self, x, enc_output, training, look_ahead_mask, padding_mask):\n",
        "        attn1, attn_weights_block1 = self.mha1(x, x, x, look_ahead_mask)\n",
        "        attn1 = self.dropout1(attn1, training=training)\n",
        "        out1 = self.layernorm1(attn1 + x)\n",
        "\n",
        "        attn2, attn_weights_block2 = self.mha2(enc_output, enc_output, out1, padding_mask)\n",
        "        attn2 = self.dropout2(attn2, training=training)\n",
        "        out2 = self.layernorm2(attn2 + out1)\n",
        "\n",
        "        ffn_output = self.ffn(out2)\n",
        "        ffn_output = self.dropout3(ffn_output, training=training)\n",
        "        out3 = self.layernorm3(ffn_output + out2)\n",
        "\n",
        "        return out3, attn_weights_block1, attn_weights_block2"
      ],
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4q3hMg377kzL"
      },
      "source": [
        "class Encoder(tf.keras.layers.Layer):\n",
        "    def __init__(self, num_layers, d_model, num_heads, dff, input_vocab_size, maximum_position_encoding, rate=0.1):\n",
        "        super(Encoder, self).__init__()\n",
        "\n",
        "        self.d_model = d_model\n",
        "        self.num_layers = num_layers\n",
        "\n",
        "        self.embedding = tf.keras.layers.Embedding(input_vocab_size, d_model)\n",
        "        self.pos_encoding = positional_encoding(maximum_position_encoding, self.d_model)\n",
        "\n",
        "        self.enc_layers = [EncoderLayer(d_model, num_heads, dff, rate) for _ in range(num_layers)]\n",
        "\n",
        "        self.dropout = tf.keras.layers.Dropout(rate)\n",
        "        \n",
        "    def call(self, x, training, mask):\n",
        "        seq_len = tf.shape(x)[1]\n",
        "\n",
        "        x = self.embedding(x)\n",
        "        x *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n",
        "        x += self.pos_encoding[:, :seq_len, :]\n",
        "\n",
        "        x = self.dropout(x, training=training)\n",
        "    \n",
        "        for i in range(self.num_layers):\n",
        "            x = self.enc_layers[i](x, training, mask)\n",
        "    \n",
        "        return x"
      ],
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_K-692nd7nRN"
      },
      "source": [
        "class Decoder(tf.keras.layers.Layer):\n",
        "    def __init__(self, num_layers, d_model, num_heads, dff, target_vocab_size, maximum_position_encoding, rate=0.1):\n",
        "        super(Decoder, self).__init__()\n",
        "\n",
        "        self.d_model = d_model\n",
        "        self.num_layers = num_layers\n",
        "\n",
        "        self.embedding = tf.keras.layers.Embedding(target_vocab_size, d_model)\n",
        "        self.pos_encoding = positional_encoding(maximum_position_encoding, d_model)\n",
        "\n",
        "        self.dec_layers = [DecoderLayer(d_model, num_heads, dff, rate) for _ in range(num_layers)]\n",
        "        self.dropout = tf.keras.layers.Dropout(rate)\n",
        "    \n",
        "    def call(self, x, enc_output, training, look_ahead_mask, padding_mask):\n",
        "        seq_len = tf.shape(x)[1]\n",
        "        attention_weights = {}\n",
        "\n",
        "        x = self.embedding(x)\n",
        "        x *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n",
        "        x += self.pos_encoding[:, :seq_len, :]\n",
        "\n",
        "        x = self.dropout(x, training=training)\n",
        "\n",
        "        for i in range(self.num_layers):\n",
        "            x, block1, block2 = self.dec_layers[i](x, enc_output, training, look_ahead_mask, padding_mask)\n",
        "\n",
        "            attention_weights['decoder_layer{}_block1'.format(i+1)] = block1\n",
        "            attention_weights['decoder_layer{}_block2'.format(i+1)] = block2\n",
        "    \n",
        "        return x, attention_weights"
      ],
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SgkktCmH7p8_"
      },
      "source": [
        "class Transformer(tf.keras.Model):\n",
        "    def __init__(self, num_layers, d_model, num_heads, dff, input_vocab_size, target_vocab_size, pe_input, pe_target, rate=0.1):\n",
        "        super(Transformer, self).__init__()\n",
        "\n",
        "        self.encoder = Encoder(num_layers, d_model, num_heads, dff, input_vocab_size, pe_input, rate)\n",
        "\n",
        "        self.decoder = Decoder(num_layers, d_model, num_heads, dff, target_vocab_size, pe_target, rate)\n",
        "\n",
        "        self.final_layer = tf.keras.layers.Dense(target_vocab_size)\n",
        "    \n",
        "    def call(self, inp, tar, training, enc_padding_mask, look_ahead_mask, dec_padding_mask):\n",
        "        enc_output = self.encoder(inp, training, enc_padding_mask)\n",
        "\n",
        "        dec_output, attention_weights = self.decoder(tar, enc_output, training, look_ahead_mask, dec_padding_mask)\n",
        "\n",
        "        final_output = self.final_layer(dec_output)\n",
        "\n",
        "        return final_output, attention_weights"
      ],
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uu3i6X4Z7tJx"
      },
      "source": [
        "num_layers = 5\n",
        "d_model = 128\n",
        "dff = 512\n",
        "num_heads = 8\n",
        "EPOCHS = 15"
      ],
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HOr_ZYeh7xnU"
      },
      "source": [
        "class CustomSchedule(tf.keras.optimizers.schedules.LearningRateSchedule):\n",
        "    def __init__(self, d_model, warmup_steps=4000):\n",
        "        super(CustomSchedule, self).__init__()\n",
        "\n",
        "        self.d_model = d_model\n",
        "        self.d_model = tf.cast(self.d_model, tf.float32)\n",
        "\n",
        "        self.warmup_steps = warmup_steps\n",
        "    \n",
        "    def __call__(self, step):\n",
        "        arg1 = tf.math.rsqrt(step)\n",
        "        arg2 = step * (self.warmup_steps ** -1.5)\n",
        "\n",
        "        return tf.math.rsqrt(self.d_model) * tf.math.minimum(arg1, arg2)"
      ],
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_v9rVV1k71F6"
      },
      "source": [
        "learning_rate = CustomSchedule(d_model)\n",
        "\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate, beta_1=0.9, beta_2=0.98, epsilon=1e-9)"
      ],
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rAzfbEmv72xj"
      },
      "source": [
        "\n",
        "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True, reduction='none')"
      ],
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3rmihCY974Ti"
      },
      "source": [
        "def loss_function(real, pred):\n",
        "    mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
        "    loss_ = loss_object(real, pred)\n",
        "\n",
        "    mask = tf.cast(mask, dtype=loss_.dtype)\n",
        "    loss_ *= mask\n",
        "\n",
        "    return tf.reduce_sum(loss_)/tf.reduce_sum(mask)"
      ],
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pMeAOFbZ77i4"
      },
      "source": [
        "\n",
        "train_loss = tf.keras.metrics.Mean(name='train_loss')"
      ],
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tpb8uCkN79gC"
      },
      "source": [
        "\n",
        "transformer = Transformer(\n",
        "    num_layers, \n",
        "    d_model, \n",
        "    num_heads, \n",
        "    dff,\n",
        "    encoder_vocab_size, \n",
        "    decoder_vocab_size, \n",
        "    pe_input=encoder_vocab_size, \n",
        "    pe_target=decoder_vocab_size,\n",
        ")"
      ],
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3xIsWj0O7-5i"
      },
      "source": [
        "def create_masks(inp, tar):\n",
        "    enc_padding_mask = create_padding_mask(inp)\n",
        "    dec_padding_mask = create_padding_mask(inp)\n",
        "\n",
        "    look_ahead_mask = create_look_ahead_mask(tf.shape(tar)[1])\n",
        "    dec_target_padding_mask = create_padding_mask(tar)\n",
        "    combined_mask = tf.maximum(dec_target_padding_mask, look_ahead_mask)\n",
        "  \n",
        "    return enc_padding_mask, combined_mask, dec_padding_mask"
      ],
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GQcD1KNp8B-C"
      },
      "source": [
        "checkpoint_path = \"checkpoints\"\n",
        "\n",
        "ckpt = tf.train.Checkpoint(transformer=transformer, optimizer=optimizer)\n",
        "\n",
        "ckpt_manager = tf.train.CheckpointManager(ckpt, checkpoint_path, max_to_keep=5)\n",
        "\n",
        "if ckpt_manager.latest_checkpoint:\n",
        "    ckpt.restore(ckpt_manager.latest_checkpoint)\n",
        "    print ('Latest checkpoint restored!!')"
      ],
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pW5rIIEW8EsI"
      },
      "source": [
        "@tf.function\n",
        "def train_step(inp, tar):\n",
        "    tar_inp = tar[:, :-1]\n",
        "    tar_real = tar[:, 1:]\n",
        "\n",
        "    enc_padding_mask, combined_mask, dec_padding_mask = create_masks(inp, tar_inp)\n",
        "\n",
        "    with tf.GradientTape() as tape:\n",
        "        predictions, _ = transformer(\n",
        "            inp, tar_inp, \n",
        "            True, \n",
        "            enc_padding_mask, \n",
        "            combined_mask, \n",
        "            dec_padding_mask\n",
        "        )\n",
        "        loss = loss_function(tar_real, predictions)\n",
        "\n",
        "    gradients = tape.gradient(loss, transformer.trainable_variables)    \n",
        "    optimizer.apply_gradients(zip(gradients, transformer.trainable_variables))\n",
        "\n",
        "    train_loss(loss)"
      ],
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9P7rmGxz8H7g",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6fcd38ff-1c9c-4c97-8fa7-b59bb4a3b9e9"
      },
      "source": [
        "\n",
        "for epoch in range(EPOCHS):\n",
        "    start = time.time()\n",
        "\n",
        "    train_loss.reset_states()\n",
        "  \n",
        "    for (batch, (inp, tar)) in enumerate(dataset):\n",
        "        train_step(inp, tar)\n",
        "    \n",
        "        \n",
        "        if batch % 500 == 0:\n",
        "            print ('Epoch {} Batch {} Loss {:.4f}'.format(epoch + 1, batch, train_loss.result()))\n",
        "      \n",
        "    if (epoch + 1) % 5 == 0:\n",
        "        ckpt_save_path = ckpt_manager.save()\n",
        "        print ('Saving checkpoint for epoch {} at {}'.format(epoch+1, ckpt_save_path))\n",
        "    \n",
        "    print ('Epoch {} Loss {:.4f}'.format(epoch + 1, train_loss.result()))\n",
        "\n",
        "    print ('Time taken for 1 epoch: {} secs\\n'.format(time.time() - start))"
      ],
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1 Batch 0 Loss 9.4283\n",
            "Epoch 1 Loss 8.3286\n",
            "Time taken for 1 epoch: 224.38176918029785 secs\n",
            "\n",
            "Epoch 2 Batch 0 Loss 6.9750\n",
            "Epoch 2 Loss 6.9667\n",
            "Time taken for 1 epoch: 220.9218487739563 secs\n",
            "\n",
            "Epoch 3 Batch 0 Loss 6.7831\n",
            "Epoch 3 Loss 6.5721\n",
            "Time taken for 1 epoch: 220.34934759140015 secs\n",
            "\n",
            "Epoch 4 Batch 0 Loss 6.4423\n",
            "Epoch 4 Loss 6.2408\n",
            "Time taken for 1 epoch: 220.37262105941772 secs\n",
            "\n",
            "Epoch 5 Batch 0 Loss 5.9554\n",
            "Saving checkpoint for epoch 5 at checkpoints/ckpt-1\n",
            "Epoch 5 Loss 5.9465\n",
            "Time taken for 1 epoch: 221.00017881393433 secs\n",
            "\n",
            "Epoch 6 Batch 0 Loss 5.9558\n",
            "Epoch 6 Loss 5.7174\n",
            "Time taken for 1 epoch: 220.41264367103577 secs\n",
            "\n",
            "Epoch 7 Batch 0 Loss 5.4987\n",
            "Epoch 7 Loss 5.5220\n",
            "Time taken for 1 epoch: 220.1197109222412 secs\n",
            "\n",
            "Epoch 8 Batch 0 Loss 4.8456\n",
            "Epoch 8 Loss 5.3359\n",
            "Time taken for 1 epoch: 220.23985362052917 secs\n",
            "\n",
            "Epoch 9 Batch 0 Loss 4.9768\n",
            "Epoch 9 Loss 5.1643\n",
            "Time taken for 1 epoch: 220.2991442680359 secs\n",
            "\n",
            "Epoch 10 Batch 0 Loss 4.8582\n",
            "Saving checkpoint for epoch 10 at checkpoints/ckpt-2\n",
            "Epoch 10 Loss 4.9896\n",
            "Time taken for 1 epoch: 220.77564978599548 secs\n",
            "\n",
            "Epoch 11 Batch 0 Loss 4.9518\n",
            "Epoch 11 Loss 4.7675\n",
            "Time taken for 1 epoch: 220.1270091533661 secs\n",
            "\n",
            "Epoch 12 Batch 0 Loss 4.3705\n",
            "Epoch 12 Loss 4.5580\n",
            "Time taken for 1 epoch: 220.09405660629272 secs\n",
            "\n",
            "Epoch 13 Batch 0 Loss 3.9036\n",
            "Epoch 13 Loss 4.3743\n",
            "Time taken for 1 epoch: 220.2093243598938 secs\n",
            "\n",
            "Epoch 14 Batch 0 Loss 3.6947\n",
            "Epoch 14 Loss 4.1886\n",
            "Time taken for 1 epoch: 220.1403272151947 secs\n",
            "\n",
            "Epoch 15 Batch 0 Loss 3.4997\n",
            "Saving checkpoint for epoch 15 at checkpoints/ckpt-3\n",
            "Epoch 15 Loss 4.0127\n",
            "Time taken for 1 epoch: 220.67403626441956 secs\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "98gwJABQXXCx",
        "outputId": "9c2ebc7c-af1f-46c2-a43b-73f1e309d36a"
      },
      "source": [
        "\n",
        "transformer.summary()"
      ],
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"transformer\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "encoder (Encoder)            multiple                  7405568   \n",
            "_________________________________________________________________\n",
            "decoder (Decoder)            multiple                  2990720   \n",
            "_________________________________________________________________\n",
            "dense_80 (Dense)             multiple                  1680870   \n",
            "=================================================================\n",
            "Total params: 12,077,158\n",
            "Trainable params: 12,077,158\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "thf1XTJJ8MLg"
      },
      "source": [
        "def evaluate(input_document):\n",
        "    input_document = document_tokenizer.texts_to_sequences([input_document])\n",
        "    input_document = tf.keras.preprocessing.sequence.pad_sequences(input_document, maxlen=encoder_maxlen, padding='post', truncating='post')\n",
        "\n",
        "    encoder_input = tf.expand_dims(input_document[0], 0)\n",
        "\n",
        "    decoder_input = [summary_tokenizer.word_index[\"<go>\"]]\n",
        "    output = tf.expand_dims(decoder_input, 0)\n",
        "    \n",
        "    for i in range(decoder_maxlen):\n",
        "        enc_padding_mask, combined_mask, dec_padding_mask = create_masks(encoder_input, output)\n",
        "\n",
        "        predictions, attention_weights = transformer(\n",
        "            encoder_input, \n",
        "            output,\n",
        "            False,\n",
        "            enc_padding_mask,\n",
        "            combined_mask,\n",
        "            dec_padding_mask\n",
        "        )\n",
        "\n",
        "        predictions = predictions[: ,-1:, :]\n",
        "        predicted_id = tf.cast(tf.argmax(predictions, axis=-1), tf.int32)\n",
        "\n",
        "        if predicted_id == summary_tokenizer.word_index[\"<stop>\"]:\n",
        "            return tf.squeeze(output, axis=0), attention_weights\n",
        "\n",
        "        output = tf.concat([output, predicted_id], axis=-1)\n",
        "\n",
        "    return tf.squeeze(output, axis=0), attention_weights"
      ],
      "execution_count": 62,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C-mk6HTe8SoT"
      },
      "source": [
        "def summarize(input_document):\n",
        "    # not considering attention weights for now, can be used to plot attention heatmaps in the future\n",
        "    summarized = evaluate(input_document=input_document)[0].numpy()\n",
        "    summarized = np.expand_dims(summarized[1:], 0)  # not printing <go> token\n",
        "    return summary_tokenizer.sequences_to_texts(summarized)[0]  # since there is just one translated document"
      ],
      "execution_count": 71,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sbTS0sF98Vit",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cd51ca40-36f3-406d-9455-9dd0b29d4f58"
      },
      "source": [
        "x=summarize(\n",
        "    df.iloc[20].abstract\n",
        ")\n",
        "print(x)\n",
        "print(df.iloc[20].title)"
      ],
      "execution_count": 94,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "the role of the cellular proteome of the human end gene expression in the human end region of the human immunodeficiency virus\n",
            "Protein secretion in Lactococcus lactis : an efficient way to increase the overall heterologous protein production\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}