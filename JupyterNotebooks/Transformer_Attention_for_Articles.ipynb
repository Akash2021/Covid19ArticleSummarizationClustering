{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Transformer Attention for Articles.ipynb",
      "provenance": [],
      "mount_file_id": "1WBJjQxljlZXf4GVlrHIaC5JzmaPh39eZ",
      "authorship_tag": "ABX9TyNAjf7p+R2gZeE3NNLP7FF/"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "jCumT6vn4rTy"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import time\n",
        "import re\n",
        "import pickle"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 323
        },
        "id": "wdQgbAtM4wLw",
        "outputId": "f423b6d1-a153-41ae-9420-4512f9a86589"
      },
      "source": [
        "import pandas as pd\n",
        "df=pd.read_csv('/content/drive/MyDrive/CovidArticles.csv')\n",
        "df.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>paper_id</th>\n",
              "      <th>title</th>\n",
              "      <th>abstract</th>\n",
              "      <th>body_text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>c79ce955bfc71ffe8159bca6bc81d783a86d8edf</td>\n",
              "      <td>A cross-sectional community-based observationa...</td>\n",
              "      <td>The Asymptomatic novel CORonavirus iNfection (...</td>\n",
              "      <td>The World Health Organization first declared a...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>97acf17564239fe170cc68c9514daf5909f7d999</td>\n",
              "      <td>Journal Pre-proof Inhaled corticosteroids down...</td>\n",
              "      <td>Singanayagam A, Inhaled corticosteroids downre...</td>\n",
              "      <td>widely used in COPD but the extent to which th...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>9e277d3a38ec5bf379b333d675963086ee32e9f2</td>\n",
              "      <td>KlINISCHE lES Bof, terug van weggeweest</td>\n",
              "      <td>Bof is een ziekte die vroeger bijna alle kinde...</td>\n",
              "      <td>Patiënt A was een volledig tegen bof gevaccine...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3cfc7674f189b49a38eacfb52d12f6e3d5d36f98</td>\n",
              "      <td>R E V I E W Coronavirus infections and immune ...</td>\n",
              "      <td>Coronaviruses (CoVs) are by far the largest gr...</td>\n",
              "      <td>The host innate immune system detects viral in...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>b5bbb2b61abecb00e772bfb00d260939f36f9e6a</td>\n",
              "      <td>International mass gatherings and travel-assoc...</td>\n",
              "      <td>Background: Travelers to international mass ga...</td>\n",
              "      <td>Attendance at an international mass gathering ...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                   paper_id  ...                                          body_text\n",
              "0  c79ce955bfc71ffe8159bca6bc81d783a86d8edf  ...  The World Health Organization first declared a...\n",
              "1  97acf17564239fe170cc68c9514daf5909f7d999  ...  widely used in COPD but the extent to which th...\n",
              "2  9e277d3a38ec5bf379b333d675963086ee32e9f2  ...  Patiënt A was een volledig tegen bof gevaccine...\n",
              "3  3cfc7674f189b49a38eacfb52d12f6e3d5d36f98  ...  The host innate immune system detects viral in...\n",
              "4  b5bbb2b61abecb00e772bfb00d260939f36f9e6a  ...  Attendance at an international mass gathering ...\n",
              "\n",
              "[5 rows x 4 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z7HQRuHPSLuG",
        "outputId": "85a6352a-c76b-4890-aa7d-339ace21d9ff"
      },
      "source": [
        "df.info()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 18950 entries, 0 to 18949\n",
            "Data columns (total 4 columns):\n",
            " #   Column     Non-Null Count  Dtype \n",
            "---  ------     --------------  ----- \n",
            " 0   paper_id   18950 non-null  object\n",
            " 1   title      17814 non-null  object\n",
            " 2   abstract   18950 non-null  object\n",
            " 3   body_text  18950 non-null  object\n",
            "dtypes: object(4)\n",
            "memory usage: 592.3+ KB\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OmUSsDniSsOP"
      },
      "source": [
        "df=df[:8000]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qip4jNDT41e3"
      },
      "source": [
        "document = df['body_text']\n",
        "summary = df['abstract']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YYMCbOGR5CEC",
        "outputId": "f3768480-6e34-42ac-9b51-a925b079525e"
      },
      "source": [
        "document[3], summary[3]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('The host innate immune system detects viral infections by using pattern recognition receptors (PRRs) to recognize pathogen-associated molecular patterns (PAMPs). At present, the known PRRs mainly include toll-like receptor (TLR), RIG-I-like receptor (RLR), NOD-like receptor (NLR), C-type lectin-like receptors (CLmin), and free-molecule receptors in the cytoplasm, such as cGAS, IFI16, STING, DAI, and so on.PAMPs recognized by Toll-like receptors (TLRs) include lipids, lipoproteins, proteins, and nucleic acids of the bacterial, viral, parasite, and fungal origins. 6 The recognition of PAMPs by TLRs also occurs in cell membranes, endosomes, lysosomes, and endocytolysosomes and other locations in cells. 6 Different TLRs can induce different biological responses via subsequent activation of varied adapter proteins, such as MyD88, TIRAP, TRIP, and TRAM, but these adapter proteins all share the Toll/Interleukin-1 receptor (TIR) structure. 7 MyD88 is the first identified TIR family member, which acts as an adapter protein by almost all TLRs except TLR3. It mainly activates the transcription factors NF-kB and mitogen-activated protein kinases (MAPKs) pathways to induce inflammatory factors\\' expression. 6 Unlike MyD88, TRIF is an adapter protein of TLR3 and TLR4, which activates the transcription factors IRF3 and NF-kB to induce the expression of type I interferon and immuneinflammatory factors. The function of TRAM and TIRAP is to recruit TRIF molecules to the TLR4 receptor and MyD88 to the TLR2 and TLR4 receptors. Therefore, the TLR signaling pathways are classified as the MyD88-dependent pathway, which functions to activate immuneinflammatory factors, and the TRIF-dependent pathway, which functions to activate the type I interferons and inflammatory factors. 6 After a TLR is activated by the corresponding PAMP, MyD88 recruits the busy-1 F I G U R E 1 Coronavirus particle. Coronaviruses are enveloped, nonsegmented, positive-sense single-stranded RNA virus genomes in the size ranging from 26 to 32 kilobases. The virion has a nucleocapsid composed of genomic RNA and phosphorylated nucleocapsid (N) protein, which is buried inside phospholipid bilayers and covered by the spike glycoprotein trimmer (S). The membrane (M) protein (a type III transmembrane glycoprotein) and the envelope (E) protein are located among the S proteins in the virus envelope receptor-related kinases IRAK4, IRAKI, IRAK2, and IRAK-M. IRAK4plays an important role in activating NF-kB and MAPKs downstream of MyD88. IRAK interacts with TRAF6, which causes its K-63 ubiquitination, and facilitates NEMO ubiquitination to activate NF-kB. TRIFdependent pathways activate IRF3 and NF-kB. 8, 9 In addition to activating NF-kB, TRIF-dependent pathways, they also activate IRF3 and interferon-β. 10 This process leads to T cell activation and differentiation, including the production of cytokines associated with the different T cell subsets (ie, Th17), followed by a massive release of cytokines for immune response amplification. The continued production of these mediators due to viral persistence has a negative effect on NK, and CD8 T cell activation. However, CD8 T cells produce very effective mediators to clear CoV. B, Attachment of CoV to DPP4R on the host cell through S protein leads to the appearance of genomic RNA in the cytoplasm. An immune response to dsRNA can be partially generated during CoV replication. TLR-3 sensitized by dsRNA and cascades of signaling pathways (IRFs and NF-κB activation, respectively) are activated to produce type I IFNs and proinflammatory cytokines. The production of type I IFNs is important to enhance the release of antiviral proteins for the protection of uninfected cells. Sometimes, accessory proteins of CoV can interfere with TLR-3 signaling and bind the dsRNA of CoV during replication to prevent TLR-3 activation and evade the immune response. TLR-4 might recognize S protein and lead to the activation of proinflammatory cytokines through the MyD88-dependent signaling pathway. Virus-cell interactions lead to the strong production of immune mediators. The secretion of large quantities of chemokines and cytokines (IL-1, IL-6, IL-8, IL-21, TNF-β, and MCP-1) is promoted in infected cells in response to CoV infection. These chemokines and cytokines, in turn, recruit lymphocytes and leukocytes to the site of infection. Red lines refer to inhibitory effects. Green lines refer to activating effects nucleocapsid proteins containing triphosphine RNA at the 5′-end can be recognized by RIG-I. 17 The double-stranded RNA with doublebasic acid at the 5′-end can be recognized by RIG-I. 18 When the viral 5′-terminal triphosphate end is recognized by the CTD structure, the ATP-dependent conformational change brings the CTD structure to form a complex with the double-stranded RNA, and the CARD structure is then released from its self-inhibition and interacts with MAVS. 19 MDA5 recognizes RNAs of picornaviruses, including poliovirus (PV) and Encephalomyocarditis virus (EMCV). MDAS-recognized RNA is characterized by long double-stranded RNA more than 1kbp. Crystal structure analysis shows that the helicase and CTD structure of MDA5 are also surrounded by double-stranded RNA, the same as RIG-I. However, the CTD structure of MDA5 does not have a hat structure, 20 and the hat structure is necessary to have a triphosphate RNA interaction at the 5′-end. The CTD structure of MDA5 directly interacts with the double-stranded RNA, so that the 5′-end RNA can be freely released. 21 Nucleotide-binding and oligomerization domain (NOD)-like receptors (NLRs) are a class of pattern recognition receptors, 22 which recognize components of pathogens and contain a conserved NOD structure. 23 NLR receptor family members are divided into three subclasses according to their functions. The first NLR subclass forms complexes with a variety of proteins and these complexes are defined as inflammasome that contains at least eight NLR proteins, including NLRP1, NLRP3, NLRP6, NLRC4, NLRC5W, and AY2. [24] [25] [26] The second subclass is essential to reproduction and embryo regeneration. 27 The third subclass is comprised of regulatory NLRs. These NLRs are positive or negative conditioned inflammatory signaling cascade pathways. When a virus invades the host, PRRs initially recognize the viral nucleic acid, collect the specific signal adapter protein, activate IRF3 and IRF7 before being translocated to the nucleus and promote the synthesis of type I interferons (IFNs). Type I IFNs subsequently activate the downstream JAK-STAT signal pathway, promote the expression of IFN-stimulated genes (ISGs). 37, 38 As the host\\'s major antiviral molecules, IFNs limit virus spread, and play an immunomodulatory role to promote macrophage pha- RIG-I/NF-κB-dependent pathway 45 and the disorder of the JAK-STAT signaling pathway directly affects the spread of virus. 46 Although SARS-CoV and other coronaviruses are sensitive to IFN-a/b, these viruses remain highly pathogenic. Reportedly, the N protein of SARS-CoV acts as an antagonist of immune escape protein and host interferon response. [47] [48] [49] It is reported that EV71 infection downregulates JAK1, p-JAK1, and p-TYK2, inhibits p-STAT1/2, and blocks the JAK-STAT signaling pathway mediated by type I IFNs, thereby hindering the function of IFNs and promoting EV71 replication and proliferation in host cells. 50 Ebola virus (EBOV) promotes cytokine signal inhibitory factor-1 (SOCS1) and blocks the JAK-STAT signal pathway by directly binding to phosphorylated JAK, resulting in the inhibition of JAK activation. 51 In addition, influenza A virus can inhibit the IFN-I downstream pathway by inducing the expression of SOCS3. 52 Defensins are a family of endogenous antibiotic peptide molecules, which widely exist in human, animals, and plants, and are important for the host\\'s innate defense system. Defensins have broad-spectrum antimicrobial activities. In vitro inhibition experiments show that defensins have killing effects on bacteria, fungi, mycoplasma, chlamydia, spirochetes, tumor cells, and viruses. 60, 61 Defensins of human and rabbit neutrophils are mainly found in the eosinophilic granules of neutrophils. They are small molecular cationic polypeptides composed of 29 to 34 amino acid residues, with a relative molecular weight of 3500 to 4000 dolt and three intramolecular disulfide bonds. They are main components of the neutrophils independent of oxygen sterilization. 62, 63 Human α-defensin HNP-1 inactivates herpes simplex virus type I and type II (HSV-1 and HSV-2), cytomegalovirus (CMV), VSV, and IAV. 64, 65 Purified defensins of guinea pigs, rabbits, and rats have weak anti-HIV-1 activity. 66, 67 However, some studies showed that purified human neutrophil defensin (HNP1-3) and rabbit neutrophil defensins (RNP1-5) could neither inhibit nor kill SARS-CoV. 68 The reappearance of SARS-CoV is still a noteworthy problem. B cell subsets with phenotypes characteristic of naive, non-isotype-CoVs. 85 The antigen stimulation of MERS-CoV infection was clarified by using the specific 9-mer peptide \"CYSSLILDY\", which located at position 437 to 445 within the region of the S glycoprotein. 85 The sequence has the highest B cell antigenicity plot and has the ability to form the greatest number of interactions with MHCI alleles in a computerized simulation. 86 Reports show that humoral immunity is essential to control the persistent phase of CoV infection. More antibodies isolated from patients who have survived MERS-CoV infection have been described, including MCA1, CDC-C2, CSC-C5, CDC-A2, CDC-A10, MERS-GD27, and MERS-GD33. [87] [88] [89] The complement system plays a vital role in the host immune response to CoV infection. Primitively identified as a host-sensitive and nonspecific complement to adaptive immune pathways, the complement system provides a way for the innate immune system to detect and respond to foreign antigens. 90 Given its potential to damage the host tissues, the complement system is tightly controlled by inhibiting proteins in the serum. Virus encoded proteins help them evade the detection of the complement system, suggesting that complements are vital to the antiviral response. C3a and C5a have potent proinflammatory properties and can trigger inflammatory cell recruitment and neutrophil activation.C3a and C5a blockade acts as a treatment for acute lung injury, and anti-C5a antibody shows to protect mice from infection with MERS-CoV. 91 SARA-CoV infection activates the complement pathway and complement signaling contributes to disease. 92 The antibody response in vivo is a dynamic and complex mixture of monoclonal antibodies (mAbs), which work together to target different antigenic domains on the envelope glycoprotein of the virus. It is important to determine whether the antibodies are powerful in the adaptive immune responses to MERS-CoV infection. Research from all over the world have described more than 20 kinds of monoclonal antibodies, most of which are human or humanized antibodies. The virus uses its spike proteins as an adhesion factor to facilitate host entry through a special receptor called dipeptidyl peptidase-4 (DPP4). This receptor is considered a key factor in the signal transmission and activation of acquired and innate immune responses in infected patients. Thus, compared with the time-consuming vaccine preparation, the design of monoclonal antibodies against these proteins has a better protective effect.Human monoclonal antibody (m336) isolated from the phage display library interacts with the receptor-binding region of MES coronavirus spike protein and displays strong neutralization activity to MES-CoV in vitro. 93 Human monoclonal antibody m336 shows high neutralization activity to MERS-CoV in vitro. m336 reduces the RNA titer of lung by 40 000 to 90 000 folds. 94 After infection with MERS-CoV, monkeys were treated with high-titer hyperimmune plasma or monoclonal antibody m336. Both groups had relieved symptoms of clinical diseases, but the reduction of respiratory viral load was only found in the hyperimmune plasma group. Although both super immune plasma and m336 therapy show to mitigate the disease of the common marmoset, neither has the ability to prevent the disease completely. 95 Yet, HMab m336 is found to significantly reduce the viral RNA titers and viral-associated pathological changes in rabbit lung tissue. 94 Mice inoculated with S nanoparticles produced high-level neutralizing antibodies against homologous viruses, and these antibodies have no cross-protection with heteroviruses. 96 After being stimulated by SARS-CoV, immunized ferrets produced more rapid and stronger neutralizing antibody reaction than the control animals; however, the strong inflammatory reaction is observed in liver tissue. All this suggests that the expression of SARS-CoV S protein is associated with enhanced hepatitis. 97 On the other hand, the time course of SARS-CoV viremia and antibody response has been studied. 98 ',\n",
              " 'Coronaviruses (CoVs) are by far the largest group of known positive-sense RNA viruses having an extensive range of natural hosts. In the past few decades, newly evolved Coronaviruses have posed a global threat to public health. The immune response is essential to control and eliminate CoV infections, however, maladjusted immune responses may result in immunopathology and impaired pulmonary gas exchange. Gaining a deeper understanding of the interaction between Coronaviruses and the innate immune systems of the hosts may shed light on the development and persistence of inflammation in the lungs and hopefully can reduce the risk of lung inflammation caused by CoVs. In this review, we provide an update on CoV infections and relevant diseases, particularly the host defense against CoV-induced inflammation of lung tissue, as well as the role of the innate immune system in the pathogenesis and clinical treatment.')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h-qHXIcy5GvW",
        "outputId": "c367c225-3f25-4e3a-ddbd-9afe944b1d87"
      },
      "source": [
        "summary = summary.apply(lambda x: '<go> ' + x + ' <stop>')\n",
        "summary.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0    <go> The Asymptomatic novel CORonavirus iNfect...\n",
              "1    <go> Singanayagam A, Inhaled corticosteroids d...\n",
              "2    <go> Bof is een ziekte die vroeger bijna alle ...\n",
              "3    <go> Coronaviruses (CoVs) are by far the large...\n",
              "4    <go> Background: Travelers to international ma...\n",
              "Name: abstract, dtype: object"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fm8mqzcI5Tdq"
      },
      "source": [
        "# since < and > from default tokens cannot be removed\n",
        "filters = '!\"#$%&()*+,-./:;=?@[\\\\]^_`{|}~\\t\\n'\n",
        "oov_token = '<unk>'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PGjwQ9yn5XYj"
      },
      "source": [
        "document_tokenizer = tf.keras.preprocessing.text.Tokenizer(oov_token=oov_token)\n",
        "summary_tokenizer = tf.keras.preprocessing.text.Tokenizer(filters=filters, oov_token=oov_token)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A-a8cGIM5aN_"
      },
      "source": [
        "document_tokenizer.fit_on_texts(document)\n",
        "summary_tokenizer.fit_on_texts(summary)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HHoFzlKF5fFK"
      },
      "source": [
        "\n",
        "inputs = document_tokenizer.texts_to_sequences(document)\n",
        "targets = summary_tokenizer.texts_to_sequences(summary)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-dap8PjG5iB4",
        "outputId": "fa06f30a-3e3d-4072-f559-acb0fdd10186"
      },
      "source": [
        "summary_tokenizer.texts_to_sequences([\"This is a test\"])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[21, 13, 7, 159]]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o3cbowDR5kdz",
        "outputId": "e08cd0d7-4b14-4545-de95-92e2ebf7872f"
      },
      "source": [
        "summary_tokenizer.sequences_to_texts([[20, 13, 7, 158]])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['as is a while']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iLmMzTSe5nXF",
        "outputId": "0d3b09b8-f725-4ad9-83a8-9f9a8fe52512"
      },
      "source": [
        "encoder_vocab_size = len(document_tokenizer.word_index) + 1\n",
        "decoder_vocab_size = len(summary_tokenizer.word_index) + 1\n",
        "\n",
        "# vocab_size\n",
        "encoder_vocab_size, decoder_vocab_size"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(186311, 57509)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z2aWM3Uy5p_E"
      },
      "source": [
        "document_lengths = pd.Series([len(x) for x in document])\n",
        "summary_lengths = pd.Series([len(x) for x in summary])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SWqa5Ot05t6m",
        "outputId": "9ff30864-4e17-4975-d6f5-d8bc3cdd879f"
      },
      "source": [
        "document_lengths.describe()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "count     8000.000000\n",
              "mean      9449.955750\n",
              "std       3648.144679\n",
              "min        209.000000\n",
              "25%       6689.750000\n",
              "50%       9797.000000\n",
              "75%      12642.000000\n",
              "max      14997.000000\n",
              "dtype: float64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zg2g43RN5vWB",
        "outputId": "629fb88f-be73-4c71-be6e-695b092886a3"
      },
      "source": [
        "summary_lengths.describe()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "count     8000.000000\n",
              "mean      1255.746500\n",
              "std        971.663868\n",
              "min         13.000000\n",
              "25%        646.000000\n",
              "50%       1108.500000\n",
              "75%       1616.250000\n",
              "max      12396.000000\n",
              "dtype: float64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lo5yUqQg5w78"
      },
      "source": [
        "encoder_maxlen = 3000\n",
        "decoder_maxlen = 400"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fzfazI78564-"
      },
      "source": [
        "\n",
        "inputs = tf.keras.preprocessing.sequence.pad_sequences(inputs, maxlen=encoder_maxlen, padding='post', truncating='post')\n",
        "targets = tf.keras.preprocessing.sequence.pad_sequences(targets, maxlen=decoder_maxlen, padding='post', truncating='post')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XLBfgUxX6s-j"
      },
      "source": [
        "\n",
        "inputs = tf.cast(inputs, dtype=tf.int32)\n",
        "targets = tf.cast(targets, dtype=tf.int32)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XzarlMLU6899"
      },
      "source": [
        "BUFFER_SIZE = 20000\n",
        "BATCH_SIZE = 4"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OCPZFusD6_zG"
      },
      "source": [
        "dataset = tf.data.Dataset.from_tensor_slices((inputs, targets)).shuffle(BUFFER_SIZE).batch(BATCH_SIZE)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zoij2pKR7Hiu"
      },
      "source": [
        "def get_angles(position, i, d_model):\n",
        "    angle_rates = 1 / np.power(10000, (2 * (i // 2)) / np.float32(d_model))\n",
        "    return position * angle_rates"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "32ogs4Hc7KDO"
      },
      "source": [
        "def positional_encoding(position, d_model):\n",
        "    angle_rads = get_angles(\n",
        "        np.arange(position)[:, np.newaxis],\n",
        "        np.arange(d_model)[np.newaxis, :],\n",
        "        d_model\n",
        "    )\n",
        "\n",
        "    # apply sin to even indices in the array; 2i\n",
        "    angle_rads[:, 0::2] = np.sin(angle_rads[:, 0::2])\n",
        "\n",
        "    # apply cos to odd indices in the array; 2i+1\n",
        "    angle_rads[:, 1::2] = np.cos(angle_rads[:, 1::2])\n",
        "\n",
        "    pos_encoding = angle_rads[np.newaxis, ...]\n",
        "\n",
        "    return tf.cast(pos_encoding, dtype=tf.float32)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oDIjp5QO7N9_"
      },
      "source": [
        "def create_padding_mask(seq):\n",
        "    seq = tf.cast(tf.math.equal(seq, 0), tf.float32)\n",
        "    return seq[:, tf.newaxis, tf.newaxis, :]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ukKhmxF17QdP"
      },
      "source": [
        "def create_look_ahead_mask(size):\n",
        "    mask = 1 - tf.linalg.band_part(tf.ones((size, size)), -1, 0)\n",
        "    return mask"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DbiZN34f7U9M"
      },
      "source": [
        "def scaled_dot_product_attention(q, k, v, mask):\n",
        "    matmul_qk = tf.matmul(q, k, transpose_b=True)\n",
        "\n",
        "    dk = tf.cast(tf.shape(k)[-1], tf.float32)\n",
        "    scaled_attention_logits = matmul_qk / tf.math.sqrt(dk)\n",
        "\n",
        "    if mask is not None:\n",
        "        scaled_attention_logits += (mask * -1e9)  \n",
        "\n",
        "    attention_weights = tf.nn.softmax(scaled_attention_logits, axis=-1)\n",
        "\n",
        "    output = tf.matmul(attention_weights, v)\n",
        "    return output, attention_weights"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yVte0TdD7Y_p"
      },
      "source": [
        "class MultiHeadAttention(tf.keras.layers.Layer):\n",
        "    def __init__(self, d_model, num_heads):\n",
        "        super(MultiHeadAttention, self).__init__()\n",
        "        self.num_heads = num_heads\n",
        "        self.d_model = d_model\n",
        "\n",
        "        assert d_model % self.num_heads == 0\n",
        "\n",
        "        self.depth = d_model // self.num_heads\n",
        "\n",
        "        self.wq = tf.keras.layers.Dense(d_model)\n",
        "        self.wk = tf.keras.layers.Dense(d_model)\n",
        "        self.wv = tf.keras.layers.Dense(d_model)\n",
        "\n",
        "        self.dense = tf.keras.layers.Dense(d_model)\n",
        "        \n",
        "    def split_heads(self, x, batch_size):\n",
        "        x = tf.reshape(x, (batch_size, -1, self.num_heads, self.depth))\n",
        "        return tf.transpose(x, perm=[0, 2, 1, 3])\n",
        "    \n",
        "    def call(self, v, k, q, mask):\n",
        "        batch_size = tf.shape(q)[0]\n",
        "\n",
        "        q = self.wq(q)\n",
        "        k = self.wk(k)\n",
        "        v = self.wv(v)\n",
        "\n",
        "        q = self.split_heads(q, batch_size)\n",
        "        k = self.split_heads(k, batch_size)\n",
        "        v = self.split_heads(v, batch_size)\n",
        "\n",
        "        scaled_attention, attention_weights = scaled_dot_product_attention(\n",
        "            q, k, v, mask)\n",
        "\n",
        "        scaled_attention = tf.transpose(scaled_attention, perm=[0, 2, 1, 3])\n",
        "\n",
        "        concat_attention = tf.reshape(scaled_attention, (batch_size, -1, self.d_model))\n",
        "        output = self.dense(concat_attention)\n",
        "            \n",
        "        return output, attention_weights\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "39FX7E4R7cHm"
      },
      "source": [
        "def point_wise_feed_forward_network(d_model, dff):\n",
        "    return tf.keras.Sequential([\n",
        "        tf.keras.layers.Dense(dff, activation='relu'),\n",
        "        tf.keras.layers.Dense(d_model)\n",
        "    ])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-nNJRVM07ebd"
      },
      "source": [
        "class EncoderLayer(tf.keras.layers.Layer):\n",
        "    def __init__(self, d_model, num_heads, dff, rate=0.1):\n",
        "        super(EncoderLayer, self).__init__()\n",
        "\n",
        "        self.mha = MultiHeadAttention(d_model, num_heads)\n",
        "        self.ffn = point_wise_feed_forward_network(d_model, dff)\n",
        "\n",
        "        self.layernorm1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
        "        self.layernorm2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
        "\n",
        "        self.dropout1 = tf.keras.layers.Dropout(rate)\n",
        "        self.dropout2 = tf.keras.layers.Dropout(rate)\n",
        "    \n",
        "    def call(self, x, training, mask):\n",
        "        attn_output, _ = self.mha(x, x, x, mask)\n",
        "        attn_output = self.dropout1(attn_output, training=training)\n",
        "        out1 = self.layernorm1(x + attn_output)\n",
        "\n",
        "        ffn_output = self.ffn(out1)\n",
        "        ffn_output = self.dropout2(ffn_output, training=training)\n",
        "        out2 = self.layernorm2(out1 + ffn_output)\n",
        "\n",
        "        return out2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZEJg8pJA7guR"
      },
      "source": [
        "class DecoderLayer(tf.keras.layers.Layer):\n",
        "    def __init__(self, d_model, num_heads, dff, rate=0.1):\n",
        "        super(DecoderLayer, self).__init__()\n",
        "\n",
        "        self.mha1 = MultiHeadAttention(d_model, num_heads)\n",
        "        self.mha2 = MultiHeadAttention(d_model, num_heads)\n",
        "\n",
        "        self.ffn = point_wise_feed_forward_network(d_model, dff)\n",
        "\n",
        "        self.layernorm1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
        "        self.layernorm2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
        "        self.layernorm3 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
        "\n",
        "        self.dropout1 = tf.keras.layers.Dropout(rate)\n",
        "        self.dropout2 = tf.keras.layers.Dropout(rate)\n",
        "        self.dropout3 = tf.keras.layers.Dropout(rate)\n",
        "    \n",
        "    \n",
        "    def call(self, x, enc_output, training, look_ahead_mask, padding_mask):\n",
        "        attn1, attn_weights_block1 = self.mha1(x, x, x, look_ahead_mask)\n",
        "        attn1 = self.dropout1(attn1, training=training)\n",
        "        out1 = self.layernorm1(attn1 + x)\n",
        "\n",
        "        attn2, attn_weights_block2 = self.mha2(enc_output, enc_output, out1, padding_mask)\n",
        "        attn2 = self.dropout2(attn2, training=training)\n",
        "        out2 = self.layernorm2(attn2 + out1)\n",
        "\n",
        "        ffn_output = self.ffn(out2)\n",
        "        ffn_output = self.dropout3(ffn_output, training=training)\n",
        "        out3 = self.layernorm3(ffn_output + out2)\n",
        "\n",
        "        return out3, attn_weights_block1, attn_weights_block2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4q3hMg377kzL"
      },
      "source": [
        "class Encoder(tf.keras.layers.Layer):\n",
        "    def __init__(self, num_layers, d_model, num_heads, dff, input_vocab_size, maximum_position_encoding, rate=0.1):\n",
        "        super(Encoder, self).__init__()\n",
        "\n",
        "        self.d_model = d_model\n",
        "        self.num_layers = num_layers\n",
        "\n",
        "        self.embedding = tf.keras.layers.Embedding(input_vocab_size, d_model)\n",
        "        self.pos_encoding = positional_encoding(maximum_position_encoding, self.d_model)\n",
        "\n",
        "        self.enc_layers = [EncoderLayer(d_model, num_heads, dff, rate) for _ in range(num_layers)]\n",
        "\n",
        "        self.dropout = tf.keras.layers.Dropout(rate)\n",
        "        \n",
        "    def call(self, x, training, mask):\n",
        "        seq_len = tf.shape(x)[1]\n",
        "\n",
        "        x = self.embedding(x)\n",
        "        x *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n",
        "        x += self.pos_encoding[:, :seq_len, :]\n",
        "\n",
        "        x = self.dropout(x, training=training)\n",
        "    \n",
        "        for i in range(self.num_layers):\n",
        "            x = self.enc_layers[i](x, training, mask)\n",
        "    \n",
        "        return x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_K-692nd7nRN"
      },
      "source": [
        "class Decoder(tf.keras.layers.Layer):\n",
        "    def __init__(self, num_layers, d_model, num_heads, dff, target_vocab_size, maximum_position_encoding, rate=0.1):\n",
        "        super(Decoder, self).__init__()\n",
        "\n",
        "        self.d_model = d_model\n",
        "        self.num_layers = num_layers\n",
        "\n",
        "        self.embedding = tf.keras.layers.Embedding(target_vocab_size, d_model)\n",
        "        self.pos_encoding = positional_encoding(maximum_position_encoding, d_model)\n",
        "\n",
        "        self.dec_layers = [DecoderLayer(d_model, num_heads, dff, rate) for _ in range(num_layers)]\n",
        "        self.dropout = tf.keras.layers.Dropout(rate)\n",
        "    \n",
        "    def call(self, x, enc_output, training, look_ahead_mask, padding_mask):\n",
        "        seq_len = tf.shape(x)[1]\n",
        "        attention_weights = {}\n",
        "\n",
        "        x = self.embedding(x)\n",
        "        x *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n",
        "        x += self.pos_encoding[:, :seq_len, :]\n",
        "\n",
        "        x = self.dropout(x, training=training)\n",
        "\n",
        "        for i in range(self.num_layers):\n",
        "            x, block1, block2 = self.dec_layers[i](x, enc_output, training, look_ahead_mask, padding_mask)\n",
        "\n",
        "            attention_weights['decoder_layer{}_block1'.format(i+1)] = block1\n",
        "            attention_weights['decoder_layer{}_block2'.format(i+1)] = block2\n",
        "    \n",
        "        return x, attention_weights"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SgkktCmH7p8_"
      },
      "source": [
        "class Transformer(tf.keras.Model):\n",
        "    def __init__(self, num_layers, d_model, num_heads, dff, input_vocab_size, target_vocab_size, pe_input, pe_target, rate=0.1):\n",
        "        super(Transformer, self).__init__()\n",
        "\n",
        "        self.encoder = Encoder(num_layers, d_model, num_heads, dff, input_vocab_size, pe_input, rate)\n",
        "\n",
        "        self.decoder = Decoder(num_layers, d_model, num_heads, dff, target_vocab_size, pe_target, rate)\n",
        "\n",
        "        self.final_layer = tf.keras.layers.Dense(target_vocab_size)\n",
        "    \n",
        "    def call(self, inp, tar, training, enc_padding_mask, look_ahead_mask, dec_padding_mask):\n",
        "        enc_output = self.encoder(inp, training, enc_padding_mask)\n",
        "\n",
        "        dec_output, attention_weights = self.decoder(tar, enc_output, training, look_ahead_mask, dec_padding_mask)\n",
        "\n",
        "        final_output = self.final_layer(dec_output)\n",
        "\n",
        "        return final_output, attention_weights"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uu3i6X4Z7tJx"
      },
      "source": [
        "num_layers = 5\n",
        "d_model = 128\n",
        "dff = 512\n",
        "num_heads = 8\n",
        "EPOCHS = 10"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HOr_ZYeh7xnU"
      },
      "source": [
        "class CustomSchedule(tf.keras.optimizers.schedules.LearningRateSchedule):\n",
        "    def __init__(self, d_model, warmup_steps=4000):\n",
        "        super(CustomSchedule, self).__init__()\n",
        "\n",
        "        self.d_model = d_model\n",
        "        self.d_model = tf.cast(self.d_model, tf.float32)\n",
        "\n",
        "        self.warmup_steps = warmup_steps\n",
        "    \n",
        "    def __call__(self, step):\n",
        "        arg1 = tf.math.rsqrt(step)\n",
        "        arg2 = step * (self.warmup_steps ** -1.5)\n",
        "\n",
        "        return tf.math.rsqrt(self.d_model) * tf.math.minimum(arg1, arg2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_v9rVV1k71F6"
      },
      "source": [
        "learning_rate = CustomSchedule(d_model)\n",
        "\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate, beta_1=0.9, beta_2=0.98, epsilon=1e-9)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rAzfbEmv72xj"
      },
      "source": [
        "\n",
        "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True, reduction='none')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3rmihCY974Ti"
      },
      "source": [
        "def loss_function(real, pred):\n",
        "    mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
        "    loss_ = loss_object(real, pred)\n",
        "\n",
        "    mask = tf.cast(mask, dtype=loss_.dtype)\n",
        "    loss_ *= mask\n",
        "\n",
        "    return tf.reduce_sum(loss_)/tf.reduce_sum(mask)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pMeAOFbZ77i4"
      },
      "source": [
        "\n",
        "train_loss = tf.keras.metrics.Mean(name='train_loss')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tpb8uCkN79gC"
      },
      "source": [
        "\n",
        "transformer = Transformer(\n",
        "    num_layers, \n",
        "    d_model, \n",
        "    num_heads, \n",
        "    dff,\n",
        "    encoder_vocab_size, \n",
        "    decoder_vocab_size, \n",
        "    pe_input=encoder_vocab_size, \n",
        "    pe_target=decoder_vocab_size,\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3xIsWj0O7-5i"
      },
      "source": [
        "def create_masks(inp, tar):\n",
        "    enc_padding_mask = create_padding_mask(inp)\n",
        "    dec_padding_mask = create_padding_mask(inp)\n",
        "\n",
        "    look_ahead_mask = create_look_ahead_mask(tf.shape(tar)[1])\n",
        "    dec_target_padding_mask = create_padding_mask(tar)\n",
        "    combined_mask = tf.maximum(dec_target_padding_mask, look_ahead_mask)\n",
        "  \n",
        "    return enc_padding_mask, combined_mask, dec_padding_mask"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GQcD1KNp8B-C"
      },
      "source": [
        "checkpoint_path = \"checkpoints\"\n",
        "\n",
        "ckpt = tf.train.Checkpoint(transformer=transformer, optimizer=optimizer)\n",
        "\n",
        "ckpt_manager = tf.train.CheckpointManager(ckpt, checkpoint_path, max_to_keep=5)\n",
        "\n",
        "if ckpt_manager.latest_checkpoint:\n",
        "    ckpt.restore(ckpt_manager.latest_checkpoint)\n",
        "    print ('Latest checkpoint restored!!')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pW5rIIEW8EsI"
      },
      "source": [
        "@tf.function\n",
        "def train_step(inp, tar):\n",
        "    tar_inp = tar[:, :-1]\n",
        "    tar_real = tar[:, 1:]\n",
        "\n",
        "    enc_padding_mask, combined_mask, dec_padding_mask = create_masks(inp, tar_inp)\n",
        "\n",
        "    with tf.GradientTape() as tape:\n",
        "        predictions, _ = transformer(\n",
        "            inp, tar_inp, \n",
        "            True, \n",
        "            enc_padding_mask, \n",
        "            combined_mask, \n",
        "            dec_padding_mask\n",
        "        )\n",
        "        loss = loss_function(tar_real, predictions)\n",
        "\n",
        "    gradients = tape.gradient(loss, transformer.trainable_variables)    \n",
        "    optimizer.apply_gradients(zip(gradients, transformer.trainable_variables))\n",
        "\n",
        "    train_loss(loss)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "98gwJABQXXCx",
        "outputId": "c83ffc70-7722-4699-cef5-3837d3f06850"
      },
      "source": [
        "\n",
        "transformer.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"transformer\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "encoder (Encoder)            multiple                  24839168  \n",
            "_________________________________________________________________\n",
            "decoder (Decoder)            multiple                  8684032   \n",
            "_________________________________________________________________\n",
            "dense_80 (Dense)             multiple                  7418661   \n",
            "=================================================================\n",
            "Total params: 40,941,861\n",
            "Trainable params: 40,941,861\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9P7rmGxz8H7g"
      },
      "source": [
        "\n",
        "for epoch in range(EPOCHS):\n",
        "    start = time.time()\n",
        "\n",
        "    train_loss.reset_states()\n",
        "  \n",
        "    for (batch, (inp, tar)) in enumerate(dataset):\n",
        "        train_step(inp, tar)\n",
        "    \n",
        "        # 55k samples\n",
        "        # we display 3 batch results -- 0th, middle and last one (approx)\n",
        "        # 55k / 64 ~ 858; 858 / 2 = 429\n",
        "        if batch % 429 == 0:\n",
        "            print ('Epoch {} Batch {} Loss {:.4f}'.format(epoch + 1, batch, train_loss.result()))\n",
        "      \n",
        "    if (epoch + 1) % 5 == 0:\n",
        "        ckpt_save_path = ckpt_manager.save()\n",
        "        print ('Saving checkpoint for epoch {} at {}'.format(epoch+1, ckpt_save_path))\n",
        "    \n",
        "    print ('Epoch {} Loss {:.4f}'.format(epoch + 1, train_loss.result()))\n",
        "\n",
        "    print ('Time taken for 1 epoch: {} secs\\n'.format(time.time() - start))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "thf1XTJJ8MLg"
      },
      "source": [
        "def evaluate(input_document):\n",
        "    input_document = document_tokenizer.texts_to_sequences([input_document])\n",
        "    input_document = tf.keras.preprocessing.sequence.pad_sequences(input_document, maxlen=encoder_maxlen, padding='post', truncating='post')\n",
        "\n",
        "    encoder_input = tf.expand_dims(input_document[0], 0)\n",
        "\n",
        "    decoder_input = [summary_tokenizer.word_index[\"<go>\"]]\n",
        "    output = tf.expand_dims(decoder_input, 0)\n",
        "    \n",
        "    for i in range(decoder_maxlen):\n",
        "        enc_padding_mask, combined_mask, dec_padding_mask = create_masks(encoder_input, output)\n",
        "\n",
        "        predictions, attention_weights = transformer(\n",
        "            encoder_input, \n",
        "            output,\n",
        "            False,\n",
        "            enc_padding_mask,\n",
        "            combined_mask,\n",
        "            dec_padding_mask\n",
        "        )\n",
        "\n",
        "        predictions = predictions[: ,-1:, :]\n",
        "        predicted_id = tf.cast(tf.argmax(predictions, axis=-1), tf.int32)\n",
        "\n",
        "        if predicted_id == summary_tokenizer.word_index[\"<stop>\"]:\n",
        "            return tf.squeeze(output, axis=0), attention_weights\n",
        "\n",
        "        output = tf.concat([output, predicted_id], axis=-1)\n",
        "\n",
        "    return tf.squeeze(output, axis=0), attention_weights"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C-mk6HTe8SoT"
      },
      "source": [
        "def summarize(input_document):\n",
        "    # not considering attention weights for now, can be used to plot attention heatmaps in the future\n",
        "    summarized = evaluate(input_document=input_document)[0].numpy()\n",
        "    summarized = np.expand_dims(summarized[1:], 0)  # not printing <go> token\n",
        "    return summary_tokenizer.sequences_to_texts(summarized)[0]  # since there is just one translated document"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sbTS0sF98Vit"
      },
      "source": [
        "summarize(\n",
        "    \"US-based private equity firm General Atlantic is in talks to invest about \\\n",
        "    $850 million to $950 million in Reliance Industries' digital unit Jio \\\n",
        "    Platforms, the Bloomberg reported. Saudi Arabia's $320 billion sovereign \\\n",
        "    wealth fund is reportedly also exploring a potential investment in the \\\n",
        "    Mukesh Ambani-led company. The 'Public Investment Fund' is looking to \\\n",
        "    acquire a minority stake in Jio Platforms.\"\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IkuZtmvj8Xj9"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}